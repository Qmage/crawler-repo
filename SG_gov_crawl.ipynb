{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import csv\n",
    "from lxml import html\n",
    "from lxml import etree\n",
    "import re\n",
    "from dateutil import parser\n",
    "import pandas\n",
    "import os\n",
    "import datetime\n",
    "import schedule\n",
    "import time\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent,} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def telco_data():\n",
    "    pageurl = 'http://ref.data.gov.sg/common/search.aspx?r=1&s=default&o=a&a=ASTA%2cIDA%2cIPOS%2cDOS%2cNRF%2cPDPC&theme=09&f=CSV%2cTXT%2cOTHERS%2cXLS%2cXML&count=10&page=1'\n",
    "    request = urllib.request.Request(pageurl,None,headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    htmldata = response.read()\n",
    "    tree = html.fromstring(htmldata)\n",
    "\n",
    "    names = ['3G Mobile Subscriptions','Nation-wide Service Coverage', 'Average Drop Call Rate', 'Average Success Rate Across All Cells']\n",
    "    type_field = {}\n",
    "    type_field['3G Mobile Subscriptions'] = ['Month','SubscriptionCount_thousands']\n",
    "    type_field['Nation-wide Service Coverage'] = ['Month','Percentage']\n",
    "    type_field['Average Drop Call Rate'] = ['Month','Percentage']\n",
    "    type_field['Average Success Rate Across All Cells'] = ['Month','Percentage']\n",
    "\n",
    "    for n in names:\n",
    "        csv_url = tree.xpath(\"//tr[td/strong/a[contains(text(),'%s')]]//input[contains(@name,'hdnFileURL')]/@value\" % n)[0] + '&filetype=csv'\n",
    "        csv_request = urllib.request.Request(csv_url,None,headers)\n",
    "        csv_response = urllib.request.urlopen(csv_request)\n",
    "        csv_data = csv_response.read()\n",
    "        cleaned_data = str(csv_data).split(\"\\\\r\\\\n\")\n",
    "        final_data = [t.replace('M','-').split(\"^^\") for t in cleaned_data if t.startswith('20')]\n",
    "        with open(n.replace(' ','_')+'.csv', 'w', newline='') as wf:\n",
    "            cwriter = csv.writer(wf, delimiter='|', quoting=csv.QUOTE_NONE)\n",
    "            cwriter.writerow(type_field[n])\n",
    "            for r in final_data:\n",
    "                cwriter.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zones = {\n",
    "    'C':'Central Region',\n",
    "    'N':'North Region',\n",
    "    'E':'East Region',\n",
    "    'W':'West Region',\n",
    "    'S':'South Region'\n",
    "}\n",
    "\n",
    "icons = {\n",
    "    'FD':'Fair (Day)',\n",
    "    'FN':'Fair (Night)',\n",
    "    'PC':'Partly Cloudy',\n",
    "    'CD':'Cloudy',\n",
    "    'HZ':'Hazy',\n",
    "    'WD':'Windy',\n",
    "    'RA':'Rain',\n",
    "    'PS':'Passing Showers',\n",
    "    'SH':'Showers',\n",
    "    'TS':'Thundery Showers'\n",
    "}\n",
    "\n",
    "regional_code = {\n",
    "    'NRS':'National Reporting Stations',\n",
    "    'rNO':'North Region',\n",
    "    'rSO':'South Region',\n",
    "    'rCE':'Central Region',\n",
    "    'rWE':'West Region',\n",
    "    'rEA':'East Region'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weatherForecast_3hr():\n",
    "    \n",
    "    api_url = \"http://www.nea.gov.sg/api/WebAPI?dataset=nowcast&keyref=781CF461BB6606AD28A78E343E0E4176E9DB7CA9E909F6EC\"\n",
    "    api_request = urllib.request.Request(api_url,None,headers)\n",
    "    api_response = urllib.request.urlopen(api_request)\n",
    "    api_data = api_response.read()\n",
    "    \n",
    "    root = etree.fromstring(api_data)\n",
    "    matchObj = re.match(r'FROM (.*) TO \\d?\\d:\\d\\d [AP]M(.*?)<br>', root.xpath('//issue_datentime')[0].text)\n",
    "    issue_datetime = str(parser.parse(matchObj.group(2) + ' ' + matchObj.group(1), dayfirst = True))\n",
    "    \n",
    "    filename = 'weatherForecast_3hr.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    max_date = ''\n",
    "    if file_exists:\n",
    "        df = pandas.read_csv(filename, delimiter=\"|\")\n",
    "        df['issue_datetime'] = pandas.to_datetime(df['issue_datetime'])\n",
    "        max_date = str(df['issue_datetime'].max())\n",
    "    if not file_exists or (parser.parse(max_date) < parser.parse(issue_datetime)):\n",
    "        fields = ['issue_datetime','name','zone','lon','lat','icon','forecast']\n",
    "        with open(filename, 'a', newline='') as wf:\n",
    "            writer = csv.DictWriter(wf, fieldnames=fields, delimiter='|', quoting=csv.QUOTE_NONE)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            for r in root.xpath('//weatherForecast/area'):\n",
    "                data = {}\n",
    "                for f in fields:\n",
    "                    if f == 'issue_datetime':\n",
    "                        data[f] = issue_datetime\n",
    "                    elif f == 'zone':\n",
    "                        data[f] = zones[r.get(f).strip()]\n",
    "                    else:\n",
    "                        data[f] = r.get(f).strip()\n",
    "                writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weatherForecast_12hr():\n",
    "    api_url = \"http://www.nea.gov.sg/api/WebAPI?dataset=12hrs_forecast&keyref=781CF461BB6606AD28A78E343E0E4176E9DB7CA9E909F6EC\"\n",
    "    api_request = urllib.request.Request(api_url,None,headers)\n",
    "    api_response = urllib.request.urlopen(api_request)\n",
    "    api_data = api_response.read()\n",
    "\n",
    "    root = etree.fromstring(api_data)\n",
    "\n",
    "    data = {}\n",
    "    for r in root.xpath('//item')[0]:\n",
    "        forecastDates = ['forecastIssue','forecastValidity','forecastValidityFrom','forecastValidityTill']\n",
    "        regions = ['wxmain','wxeast','wxwest','wxnorth','wxsouth','wxcentral']\n",
    "        tempHum = ['temperature','relativeHumidity']\n",
    "        tides = ['highTide','lowTide']\n",
    "\n",
    "        if r.tag in forecastDates:\n",
    "            data[r.tag] = str(parser.parse(r.get('date').strip() + ' ' + r.get('time').strip(), dayfirst = True))\n",
    "        elif r.tag in regions:\n",
    "            data[r.tag] = icons[r.text.strip()]\n",
    "        elif r.tag in tempHum:\n",
    "            data[r.tag + 'Low'] = r.get('low').strip()\n",
    "            data[r.tag + 'High'] = r.get('high').strip()\n",
    "        elif r.tag in tides:\n",
    "            data[r.tag + 'Time'] = r.get('time').strip()\n",
    "            data[r.tag + 'Height'] = r.get('height').strip()\n",
    "        elif r.tag == 'forecast':\n",
    "            data[r.tag] = r.text.strip()\n",
    "\n",
    "    filename = 'weatherForecast_12hr.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    max_date = ''\n",
    "    if file_exists:\n",
    "        df = pandas.read_csv(filename, delimiter=\"|\")\n",
    "        df['forecastIssue'] = pandas.to_datetime(df['forecastIssue'])\n",
    "        max_date = str(df['forecastIssue'].max())\n",
    "    if not file_exists or (parser.parse(max_date) < parser.parse(data['forecastIssue'])):\n",
    "        fields = ['forecastIssue','forecastValidity','forecastValidityFrom','forecastValidityTill','wxmain','wxeast','wxwest','wxnorth',\n",
    "             'wxsouth','wxcentral','forecast','temperatureLow','temperatureHigh','relativeHumidityLow','relativeHumidityHigh',\n",
    "                  'highTideTime','highTideHeight','lowTideTime','lowTideHeight']\n",
    "        with open(filename, 'a', newline='') as wf:\n",
    "            writer = csv.DictWriter(wf, fieldnames=fields, delimiter='|', quoting=csv.QUOTE_NONE)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weatherForecast_3days():\n",
    "    api_url = \"http://www.nea.gov.sg/api/WebAPI?dataset=3days_outlook&keyref=781CF461BB6606AD28A78E343E0E4176E9DB7CA9E909F6EC\"\n",
    "    api_request = urllib.request.Request(api_url,None,headers)\n",
    "    api_response = urllib.request.urlopen(api_request)\n",
    "    api_data = api_response.read()\n",
    "\n",
    "    root = etree.fromstring(api_data)\n",
    "\n",
    "    data = {}\n",
    "    dateObj = parser.parse(root.xpath('//item/issueDate')[0].text.strip(), dayfirst = True)\n",
    "    data['issueDate'] = str(dateObj)\n",
    "\n",
    "    filename = 'weatherForecast_3days.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    max_date = ''\n",
    "    if file_exists:\n",
    "        df = pandas.read_csv(filename, delimiter=\"|\")\n",
    "        df['issueDate'] = pandas.to_datetime(df['issueDate'])\n",
    "        max_date = str(df['issueDate'].max())\n",
    "    if not file_exists or (parser.parse(max_date) < dateObj):\n",
    "        fields = ['issueDate','predictionDate','day','temperatureLow','temperatureHigh','icon','forecast']\n",
    "        with open(filename, 'a', newline='') as wf:\n",
    "            writer = csv.DictWriter(wf, fieldnames=fields, delimiter='|', quoting=csv.QUOTE_NONE)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            for item in root.xpath('//item')[0]:\n",
    "                if item.tag == 'weatherForecast':\n",
    "                    increment = 0\n",
    "                    for r in item:\n",
    "                        if r.tag == 'day':\n",
    "                            increment += 1\n",
    "                            data[r.tag] = r.text.strip()\n",
    "                            data['predictionDate'] = str(dateObj + datetime.timedelta(days=increment))\n",
    "                        elif r.tag == 'forecast':\n",
    "                            data[r.tag] = r.text.strip()\n",
    "                        elif r.tag == 'icon':\n",
    "                            data[r.tag] = icons[r.text.strip()]\n",
    "                        elif r.tag == 'temperature':\n",
    "                            data[r.tag + 'Low'] = r.get('low').strip()\n",
    "                            data[r.tag + 'High'] = r.get('high').strip()\n",
    "                            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def psi_update():\n",
    "    api_url = \"http://www.nea.gov.sg/api/WebAPI?dataset=psi_update&keyref=781CF461BB6606AD28A78E343E0E4176E9DB7CA9E909F6EC\"\n",
    "    api_request = urllib.request.Request(api_url,None,headers)\n",
    "    api_response = urllib.request.urlopen(api_request)\n",
    "    api_data = api_response.read()\n",
    "\n",
    "    root = etree.fromstring(api_data)\n",
    "\n",
    "    li = []\n",
    "    for regions in root.xpath('//item/region'):\n",
    "        data = {}\n",
    "        for item in regions:\n",
    "            if item.tag == 'id':\n",
    "                data['region'] = regional_code[item.text.strip()]\n",
    "            if item.tag in ['latitude','longitude']:\n",
    "                data[item.tag] = item.text.strip()\n",
    "            if item.tag == 'record':\n",
    "                data['timestamp'] = str(parser.parse(item.get('timestamp').strip()))\n",
    "                for r in item:\n",
    "                    data[r.get('type').strip()] = r.get('value').strip()\n",
    "        li.append(data)\n",
    "\n",
    "    filename = 'psi_update.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    max_date = ''\n",
    "    if file_exists:\n",
    "        df = pandas.read_csv(filename, delimiter=\"|\")\n",
    "        df['timestamp'] = pandas.to_datetime(df['timestamp'])\n",
    "        max_date = str(df['timestamp'].max())\n",
    "    if not file_exists or (parser.parse(max_date) < parser.parse(li[0]['timestamp'])):\n",
    "        fields = ['timestamp','region','latitude','longitude',\n",
    "                  'NPSI','NPSI_PM25_3HR','NO2_1HR_MAX','PM10_24HR','PM25_24HR',\n",
    "                  'SO2_24HR','CO_8HR_MAX','O3_8HR_MAX',\n",
    "                  'NPSI_CO','NPSI_O3','NPSI_PM10','NPSI_PM25','NPSI_SO2']\n",
    "        with open(filename, 'a', newline='') as wf:\n",
    "            writer = csv.DictWriter(wf, fieldnames=fields, delimiter='|', quoting=csv.QUOTE_NONE)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            for data in li:\n",
    "                writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pm25_update():\n",
    "    api_url = \"http://www.nea.gov.sg/api/WebAPI?dataset=pm2.5_update&keyref=781CF461BB6606AD28A78E343E0E4176E9DB7CA9E909F6EC\"\n",
    "    api_request = urllib.request.Request(api_url,None,headers)\n",
    "    api_response = urllib.request.urlopen(api_request)\n",
    "    api_data = api_response.read()\n",
    "\n",
    "    root = etree.fromstring(api_data)\n",
    "    li = []\n",
    "    for regions in root.xpath('//item/region'):\n",
    "        data = {}\n",
    "        for item in regions:\n",
    "            if item.tag == 'id':\n",
    "                data['region'] = regional_code[item.text.strip()]\n",
    "            if item.tag in ['latitude','longitude']:\n",
    "                data[item.tag] = item.text.strip()\n",
    "            if item.tag == 'record':\n",
    "                data['timestamp'] = str(parser.parse(item.get('timestamp').strip()))\n",
    "                for r in item:\n",
    "                    data[r.get('type').strip()] = r.get('value').strip()\n",
    "        li.append(data)\n",
    "\n",
    "\n",
    "    filename = 'pm2.5_update.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    max_date = ''\n",
    "    if file_exists:\n",
    "        df = pandas.read_csv(filename, delimiter=\"|\")\n",
    "        df['timestamp'] = pandas.to_datetime(df['timestamp'])\n",
    "        max_date = str(df['timestamp'].max())\n",
    "    if not file_exists or (parser.parse(max_date) < parser.parse(li[0]['timestamp'])):\n",
    "        fields = ['timestamp','region','latitude','longitude','PM25_RGN_1HR']\n",
    "        with open(filename, 'a', newline='') as wf:\n",
    "            writer = csv.DictWriter(wf, fieldnames=fields, delimiter='|', quoting=csv.QUOTE_NONE)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            for data in li:\n",
    "                writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def job():\n",
    "    print(\"Pulling Data...\")\n",
    "    try:\n",
    "        weatherForecast_3hr()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"Failed to pull data. Skipping [%s]\" % 'weatherForecast_3hr')\n",
    "    try:\n",
    "        weatherForecast_12hr()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"Failed to pull data. Skipping [%s]\" % 'weatherForecast_12hr')\n",
    "    try:\n",
    "        weatherForecast_3days()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"Failed to pull data. Skipping [%s]\" % 'weatherForecast_3days')\n",
    "    try:\n",
    "        psi_update()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"Failed to pull data. Skipping [%s]\" % 'psi_update')\n",
    "    try:\n",
    "        pm25_update()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"Failed to pull data. Skipping [%s]\" % 'pm25_update')\n",
    "    try:\n",
    "        telco_data()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"Failed to pull data. Skipping [%s]\" % 'telco_data')\n",
    "    print(\"Data pull done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling Data...\n",
      "Data pull done.\n"
     ]
    }
   ],
   "source": [
    "job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d979c6055f38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "schedule.every(30).minutes.do(job)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
